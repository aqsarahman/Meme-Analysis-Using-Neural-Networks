{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Aqsa Rahman\n",
    "## Roll no: i191908\n",
    "## Section DS-N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= pd.read_csv(\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()\n",
    "#dropping the NaN values from our dataset.\n",
    "labels=labels.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference=pd.read_csv(\"reference.csv\")\n",
    "#dropping the NaN values from our dataset.\n",
    "reference=reference.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model-(for Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],\n",
    "                    ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,\n",
    "                    num_layers=2, dropout=0.2, key_size=128, query_size=128,\n",
    "                    value_size=128, hid_in_features=128, mlm_in_features=128,\n",
    "                    nsp_in_features=128)\n",
    "devices = d2l.try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # Forward pass\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
    "                                  valid_lens_x.reshape(-1),\n",
    "                                  pred_positions_X)\n",
    "    # Compute masked language model loss\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # Compute next sentence prediction loss\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
    "                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    # Sum of masked language modeling losses, sum of next sentence prediction\n",
    "    # losses, no. of sentence pairs, count\n",
    "    metric = d2l.Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
    "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            animator.add(step + 1,\n",
    "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_bert(train_iter, net, loss, len(vocab), devices, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([ 0.2624, -0.2890,  1.0206], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding image names to a list from the dataframe\n",
    "image_new= labels['image_name']\n",
    "image_new=image_new.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_new:\n",
    "    image = Image.open(\"images/\"+i)\n",
    "    #resizing the image to 600x600\n",
    "    image = image.resize((120,120),Image.ANTIALIAS)\n",
    "    #converting to RGB AND PNG(image-type)\n",
    "    image = image.convert('RGB')\n",
    "    new_name = i.split('.')[0]+\".png\"\n",
    "    #saving the processed images in a new file\n",
    "    image.save(fp=\"new_images/\"+new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train, Test, and Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the image name data to provide specific labels to different classes.\n",
    "def label_encoding(labels):\n",
    "    labels = labels[['image_name','text_corrected','overall_sentiment']]\n",
    "    labels['image_name'] = [i.split(\".\")[0] for i in labels['image_name'] ]\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"very_positive\", 1)\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"positive\", 1)\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"very_negative\", 2)\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"negative\", 2)\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"neutral\", 0)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-87-8369cb02509a>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['image_name'] = [i.split(\".\")[0] for i in labels['image_name'] ]\n",
      "<ipython-input-87-8369cb02509a>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"very_positive\", 1)\n",
      "<ipython-input-87-8369cb02509a>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"positive\", 1)\n",
      "<ipython-input-87-8369cb02509a>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"very_negative\", 2)\n",
      "<ipython-input-87-8369cb02509a>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"negative\", 2)\n",
      "<ipython-input-87-8369cb02509a>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"neutral\", 0)\n"
     ]
    }
   ],
   "source": [
    "labels=label_encoding(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train=60,test=20,validate=20\n",
    "train, validate, test = np.split(labels.sample(frac=1, random_state=30), [int(.6*len(labels)), int(.8*len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving into new csv files.\n",
    "train.to_csv(\"train.csv\",index=False)\n",
    "validate.to_csv(\"validate.csv\",index=False)\n",
    "test.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Neural network (ANN) for images data to classify in 5 labels\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "class Dataset_loader(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        train_labels = pd.read_csv(annotations_file)\n",
    "        labels_images = [i for i in train_labels['overall_sentiment']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]+'.png')\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 2]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        text = self.img_labels.iloc[idx, 1]\n",
    "        text = text.split()\n",
    "        padding = 187 - len(text)\n",
    "        if(padding>0):\n",
    "            for i in range(padding):\n",
    "                text.append('')\n",
    "        else:\n",
    "            text = text[0:187]\n",
    "        encoded_text = get_bert_encoding(net,text)\n",
    "        array = torch.as_tensor(label)\n",
    "        x=image.float()\n",
    "        return x,encoded_text,array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader():\n",
    "    train = Dataset_loader(annotations_file=\"train.csv\",img_dir=\"./new_images\")\n",
    "    test = Dataset_loader(annotations_file=\"test.csv\",img_dir=\"./new_images\")\n",
    "    validate = Dataset_loader(annotations_file=\"validate.csv\",img_dir=\"./new_images\")\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=64,num_workers=0,pin_memory=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=64)\n",
    "    validate_dataloader = torch.utils.data.DataLoader(validate, batch_size=64)\n",
    "    return train_dataloader,test_dataloader,validate_dataloader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader,test_dataloader,validate_dataloader = loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[110., 105., 114.,  ...,  56.,  57.,  59.],\n",
       "          [103., 103., 103.,  ...,  57.,  59.,  59.],\n",
       "          [112.,  96.,  64.,  ...,  69.,  62.,  60.],\n",
       "          ...,\n",
       "          [ 37.,  38.,  43.,  ..., 102., 170., 197.],\n",
       "          [ 56.,  57.,  55.,  ..., 181., 197., 213.],\n",
       "          [ 58.,  59.,  60.,  ..., 224., 224., 225.]],\n",
       " \n",
       "         [[ 61.,  61.,  76.,  ...,  39.,  40.,  43.],\n",
       "          [ 62.,  69.,  66.,  ...,  43.,  42.,  41.],\n",
       "          [ 68.,  70.,  48.,  ...,  61.,  53.,  53.],\n",
       "          ...,\n",
       "          [ 40.,  44.,  53.,  ..., 104., 170., 198.],\n",
       "          [ 57.,  62.,  62.,  ..., 179., 195., 213.],\n",
       "          [ 61.,  63.,  65.,  ..., 227., 224., 225.]],\n",
       " \n",
       "         [[ 50.,  43.,  56.,  ...,  35.,  38.,  41.],\n",
       "          [ 47.,  50.,  53.,  ...,  38.,  39.,  37.],\n",
       "          [ 53.,  55.,  38.,  ...,  46.,  40.,  39.],\n",
       "          ...,\n",
       "          [ 37.,  41.,  54.,  ..., 112., 173., 197.],\n",
       "          [ 60.,  64.,  68.,  ..., 183., 197., 213.],\n",
       "          [ 63.,  63.,  68.,  ..., 222., 223., 225.]]]),\n",
       " tensor([[[ 0.0467,  0.6551,  0.3792,  ..., -0.3258,  0.6611,  0.5638],\n",
       "          [-0.5059,  0.7886, -0.3266,  ...,  0.2064,  0.4340,  0.0568],\n",
       "          [-0.4370,  0.1563,  1.0442,  ..., -1.3536,  0.4309, -0.5407],\n",
       "          ...,\n",
       "          [-1.2255,  0.1867, -1.3107,  ...,  0.2933,  0.6105,  0.9532],\n",
       "          [ 0.4979, -0.0221,  0.0421,  ...,  0.2486,  0.8693,  0.7671],\n",
       "          [ 0.8438,  0.2319,  0.1508,  ..., -0.7906,  0.1906,  0.3459]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Dataset_loader(annotations_file=\"test.csv\",img_dir=\"./new_images\")\n",
    "test.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Neural Network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(43200,2000)\n",
    "        self.fc2 = nn.Linear(2000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 500)\n",
    "        self.fc4 = nn.Linear(500, 100)\n",
    "        self.fc5 = nn.Linear(100, 3)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        x =F.log_softmax(self.fc5(x))\n",
    "        return x\n",
    "image_net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=43200, out_features=2000, bias=True)\n",
       "  (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "  (fc3): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (fc4): Linear(in_features=500, out_features=100, bias=True)\n",
       "  (fc5): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Text Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(24192,5000)\n",
    "        self.fc2 = nn.Linear(5000, 3000)\n",
    "        self.fc3 = nn.Linear(3000, 1500)\n",
    "        self.fc4 = nn.Linear(1500, 500)\n",
    "        self.fc5 = nn.Linear(500, 100)\n",
    "        self.fc6 = nn.Linear(100, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "\n",
    "        x =self.fc6(x)\n",
    "        return x\n",
    "nettext = TextNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joining_models(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(Joining_models, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.classifier = nn.Linear(6,3)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.model1(x1)\n",
    "        x2 = self.model2(x2)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmodel=Joining_models(image_net,nettext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(finalmodel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-118-a9cf0ed24942>:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x =F.log_softmax(self.fc5(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.534\n",
      "[1,     3] loss: 0.975\n",
      "[1,     5] loss: 1.152\n",
      "[1,     7] loss: 0.953\n",
      "[1,     9] loss: 1.019\n",
      "[1,    11] loss: 0.900\n",
      "[1,    13] loss: 0.919\n",
      "[1,    15] loss: 1.049\n",
      "[1,    17] loss: 0.981\n",
      "[1,    19] loss: 0.908\n",
      "[1,    21] loss: 0.928\n",
      "[1,    23] loss: 0.886\n",
      "[1,    25] loss: 0.921\n",
      "[1,    27] loss: 0.916\n",
      "[1,    29] loss: 0.845\n",
      "[1,    31] loss: 0.909\n",
      "[1,    33] loss: 0.885\n",
      "[1,    35] loss: 0.928\n",
      "[1,    37] loss: 0.861\n",
      "[1,    39] loss: 0.803\n",
      "[1,    41] loss: 0.837\n",
      "[1,    43] loss: 1.002\n",
      "[1,    45] loss: 0.959\n",
      "[1,    47] loss: 0.922\n",
      "[1,    49] loss: 0.875\n",
      "[1,    51] loss: 0.950\n",
      "[1,    53] loss: 0.924\n",
      "[1,    55] loss: 0.923\n",
      "[1,    57] loss: 0.845\n",
      "[1,    59] loss: 0.892\n",
      "[1,    61] loss: 0.902\n",
      "[1,    63] loss: 0.944\n",
      "[1,    65] loss: 1.406\n",
      "[2,     1] loss: 0.459\n",
      "[2,     3] loss: 0.962\n",
      "[2,     5] loss: 0.968\n",
      "[2,     7] loss: 0.887\n",
      "[2,     9] loss: 0.916\n",
      "[2,    11] loss: 0.912\n",
      "[2,    13] loss: 0.962\n",
      "[2,    15] loss: 1.064\n",
      "[2,    17] loss: 0.953\n",
      "[2,    19] loss: 0.883\n",
      "[2,    21] loss: 0.934\n",
      "[2,    23] loss: 0.903\n",
      "[2,    25] loss: 0.918\n",
      "[2,    27] loss: 0.911\n",
      "[2,    29] loss: 0.823\n",
      "[2,    31] loss: 0.891\n",
      "[2,    33] loss: 0.884\n",
      "[2,    35] loss: 0.920\n",
      "[2,    37] loss: 0.868\n",
      "[2,    39] loss: 0.787\n",
      "[2,    41] loss: 0.841\n",
      "[2,    43] loss: 0.989\n",
      "[2,    45] loss: 0.987\n",
      "[2,    47] loss: 0.936\n",
      "[2,    49] loss: 0.873\n",
      "[2,    51] loss: 0.969\n",
      "[2,    53] loss: 0.903\n",
      "[2,    55] loss: 0.949\n",
      "[2,    57] loss: 0.879\n",
      "[2,    59] loss: 0.878\n",
      "[2,    61] loss: 0.889\n",
      "[2,    63] loss: 0.970\n",
      "[2,    65] loss: 1.416\n",
      "[3,     1] loss: 0.451\n",
      "[3,     3] loss: 0.925\n",
      "[3,     5] loss: 0.997\n",
      "[3,     7] loss: 0.918\n",
      "[3,     9] loss: 0.931\n",
      "[3,    11] loss: 0.924\n",
      "[3,    13] loss: 0.944\n",
      "[3,    15] loss: 0.957\n",
      "[3,    17] loss: 0.938\n",
      "[3,    19] loss: 0.862\n",
      "[3,    21] loss: 0.942\n",
      "[3,    23] loss: 0.896\n",
      "[3,    25] loss: 0.917\n",
      "[3,    27] loss: 0.898\n",
      "[3,    29] loss: 0.819\n",
      "[3,    31] loss: 0.889\n",
      "[3,    33] loss: 0.878\n",
      "[3,    35] loss: 0.915\n",
      "[3,    37] loss: 0.863\n",
      "[3,    39] loss: 0.777\n",
      "[3,    41] loss: 0.821\n",
      "[3,    43] loss: 0.977\n",
      "[3,    45] loss: 0.955\n",
      "[3,    47] loss: 0.927\n",
      "[3,    49] loss: 0.863\n",
      "[3,    51] loss: 0.943\n",
      "[3,    53] loss: 0.903\n",
      "[3,    55] loss: 0.928\n",
      "[3,    57] loss: 0.848\n",
      "[3,    59] loss: 0.869\n",
      "[3,    61] loss: 0.870\n",
      "[3,    63] loss: 0.934\n",
      "[3,    65] loss: 1.308\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs,text, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = finalmodel(inputs,text)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 0:   \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  = 58 percent\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs,text,labels = data\n",
    "        outputs = nettext(text)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy  = {100 * correct // total } percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './Ai_project.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
