{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Aqsa Rahman\n",
    "## Roll no: i191908\n",
    "## Section DS-N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= pd.read_csv(\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()\n",
    "#dropping the NaN values from our dataset.\n",
    "labels=labels.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_name</th>\n",
       "      <th>text_ocr</th>\n",
       "      <th>text_corrected</th>\n",
       "      <th>humour</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>offensive</th>\n",
       "      <th>motivational</th>\n",
       "      <th>overall_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>image_1.jpg</td>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>general</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>image_2.jpeg</td>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>not_funny</td>\n",
       "      <td>general</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>image_3.JPG</td>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>not_sarcastic</td>\n",
       "      <td>not_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>image_4.png</td>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>very_funny</td>\n",
       "      <td>twisted_meaning</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>motivational</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>image_5.png</td>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>very_twisted</td>\n",
       "      <td>very_offensive</td>\n",
       "      <td>not_motivational</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    image_name  \\\n",
       "0           0   image_1.jpg   \n",
       "1           1  image_2.jpeg   \n",
       "2           2   image_3.JPG   \n",
       "3           3   image_4.png   \n",
       "4           4   image_5.png   \n",
       "\n",
       "                                            text_ocr  \\\n",
       "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
       "1  The best of #10 YearChallenge! Completed in le...   \n",
       "2  Sam Thorne @Strippin ( Follow Follow Saw every...   \n",
       "3              10 Year Challenge - Sweet Dee Edition   \n",
       "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n",
       "\n",
       "                                      text_corrected      humour  \\\n",
       "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   hilarious   \n",
       "1  The best of #10 YearChallenge! Completed in le...   not_funny   \n",
       "2  Sam Thorne @Strippin ( Follow Follow Saw every...  very_funny   \n",
       "3              10 Year Challenge - Sweet Dee Edition  very_funny   \n",
       "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   hilarious   \n",
       "\n",
       "           sarcasm       offensive      motivational overall_sentiment  \n",
       "0          general   not_offensive  not_motivational     very_positive  \n",
       "1          general   not_offensive      motivational     very_positive  \n",
       "2    not_sarcastic   not_offensive  not_motivational          positive  \n",
       "3  twisted_meaning  very_offensive      motivational          positive  \n",
       "4     very_twisted  very_offensive  not_motivational           neutral  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=labels[['humour','sarcasm','offensive','motivational','overall_sentiment','image_name','text_corrected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference=pd.read_csv(\"reference.csv\")\n",
    "#dropping the NaN values from our dataset.\n",
    "reference=reference.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model-(for Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],\n",
    "                    ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,\n",
    "                    num_layers=2, dropout=0.2, key_size=128, query_size=128,\n",
    "                    value_size=128, hid_in_features=128, mlm_in_features=128,\n",
    "                    nsp_in_features=128)\n",
    "devices = d2l.try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # Forward pass\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
    "                                  valid_lens_x.reshape(-1),\n",
    "                                  pred_positions_X)\n",
    "    # Compute masked language model loss\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # Compute next sentence prediction loss\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
    "                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    # Sum of masked language modeling losses, sum of next sentence prediction\n",
    "    # losses, no. of sentence pairs, count\n",
    "    metric = d2l.Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
    "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            animator.add(step + 1,\n",
    "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_bert(train_iter, net, loss, len(vocab), devices, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([ 0.7555, -2.6361, -0.0336], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding image names to a list from the dataframe\n",
    "image_new= labels['image_name']\n",
    "image_new=image_new.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_new:\n",
    "    image = Image.open(\"images/\"+i)\n",
    "    #resizing the image to 600x600\n",
    "    image = image.resize((120,120),Image.ANTIALIAS)\n",
    "    #converting to RGB AND PNG(image-type)\n",
    "    image = image.convert('RGB')\n",
    "    new_name = i.split('.')[0]+\".png\"\n",
    "    #saving the processed images in a new file\n",
    "    image.save(fp=\"new_images/\"+new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train, Test, and Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the image name data to provide specific labels to different classes.\n",
    "def label_encoding(labels):\n",
    "    labels = labels[['image_name','text_corrected','humour','sarcasm','offensive','motivational','overall_sentiment']]\n",
    "    labels['image_name'] = [i.split(\".\")[0] for i in labels['image_name'] ]\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"very_positive\", 1)\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"positive\", 1)\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"very_negative\", 2)\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"negative\", 2)\n",
    "    labels['overall_sentiment'] = labels['overall_sentiment'].replace(\"neutral\", 0)\n",
    "    labels['sarcasm'] = labels['sarcasm'].replace(\"not_sarcastic\", 0)\n",
    "    labels['sarcasm'] = labels['sarcasm'].replace(\"general\", 1)\n",
    "    labels['sarcasm'] = labels['sarcasm'].replace(\"twisted_meaning\", 1)\n",
    "    labels['sarcasm'] = labels['sarcasm'].replace(\"very_twisted\", 1)\n",
    "    labels['offensive'] = labels['offensive'].replace(\"not_offensive\", 0)\n",
    "    labels['offensive'] = labels['offensive'].replace(\"very_offensive\", 1)\n",
    "    labels['offensive'] = labels['offensive'].replace(\"slight\", 1)\n",
    "    labels['offensive'] = labels['offensive'].replace(\"hateful_offensive\", 1)\n",
    "    labels['humour'] = labels['humour'].replace(\"not_funny\", 0)\n",
    "    labels['humour'] = labels['humour'].replace(\"very_funny\", 1)\n",
    "    labels['humour'] = labels['humour'].replace(\"hilarious\", 1)\n",
    "    labels['humour'] = labels['humour'].replace(\"funny\", 1)\n",
    "    labels['motivational'] = labels['motivational'].replace(\"not_motivational\", 0)\n",
    "    labels['motivational'] = labels['motivational'].replace(\"motivational\", 1)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=label_encoding(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train=60,test=20,validate=20\n",
    "train, validate, test = np.split(labels.sample(frac=1, random_state=30), [int(.6*len(labels)), int(.8*len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving into new csv files.\n",
    "train.to_csv(\"train.csv\",index=False)\n",
    "validate.to_csv(\"validate.csv\",index=False)\n",
    "test.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Neural network (ANN) for images data to classify in 5 labels\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "class Dataset_loader(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        train_labels = pd.read_csv(annotations_file)\n",
    "        labels_images = [i for i in train_labels['overall_sentiment']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]+'.png')\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 2]\n",
    "        two=self.img_labels.iloc[idx, 3]\n",
    "        three=self.img_labels.iloc[idx, 4]\n",
    "        four=self.img_labels.iloc[idx, 5]\n",
    "        five=self.img_labels.iloc[idx, 6]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        text = self.img_labels.iloc[idx, 1]\n",
    "        text = text.split()\n",
    "        padding = 187 - len(text)\n",
    "        if(padding>0):\n",
    "            for i in range(padding):\n",
    "                text.append('')\n",
    "        else:\n",
    "            text = text[0:187]\n",
    "        encoded_text = get_bert_encoding(net,text)\n",
    "        array = torch.as_tensor(label)\n",
    "        array1 = torch.as_tensor(two)\n",
    "        array2 = torch.as_tensor(three)\n",
    "        array3 = torch.as_tensor(four)\n",
    "        array4 = torch.as_tensor(five)\n",
    "        x=image.float()\n",
    "        return x,encoded_text,array,array1,array2,array3,array4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader():\n",
    "    train = Dataset_loader(annotations_file=\"train.csv\",img_dir=\"./new_images\")\n",
    "    test = Dataset_loader(annotations_file=\"test.csv\",img_dir=\"./new_images\")\n",
    "    validate = Dataset_loader(annotations_file=\"validate.csv\",img_dir=\"./new_images\")\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=64,num_workers=0,pin_memory=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=64)\n",
    "    validate_dataloader = torch.utils.data.DataLoader(validate, batch_size=64)\n",
    "    return train_dataloader,test_dataloader,validate_dataloader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader,test_dataloader,validate_dataloader = loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[110., 105., 114.,  ...,  56.,  57.,  59.],\n",
       "          [103., 103., 103.,  ...,  57.,  59.,  59.],\n",
       "          [112.,  96.,  64.,  ...,  69.,  62.,  60.],\n",
       "          ...,\n",
       "          [ 37.,  38.,  43.,  ..., 102., 170., 197.],\n",
       "          [ 56.,  57.,  55.,  ..., 181., 197., 213.],\n",
       "          [ 58.,  59.,  60.,  ..., 224., 224., 225.]],\n",
       " \n",
       "         [[ 61.,  61.,  76.,  ...,  39.,  40.,  43.],\n",
       "          [ 62.,  69.,  66.,  ...,  43.,  42.,  41.],\n",
       "          [ 68.,  70.,  48.,  ...,  61.,  53.,  53.],\n",
       "          ...,\n",
       "          [ 40.,  44.,  53.,  ..., 104., 170., 198.],\n",
       "          [ 57.,  62.,  62.,  ..., 179., 195., 213.],\n",
       "          [ 61.,  63.,  65.,  ..., 227., 224., 225.]],\n",
       " \n",
       "         [[ 50.,  43.,  56.,  ...,  35.,  38.,  41.],\n",
       "          [ 47.,  50.,  53.,  ...,  38.,  39.,  37.],\n",
       "          [ 53.,  55.,  38.,  ...,  46.,  40.,  39.],\n",
       "          ...,\n",
       "          [ 37.,  41.,  54.,  ..., 112., 173., 197.],\n",
       "          [ 60.,  64.,  68.,  ..., 183., 197., 213.],\n",
       "          [ 63.,  63.,  68.,  ..., 222., 223., 225.]]]),\n",
       " tensor([[[ 0.0971, -0.4387,  0.8979,  ..., -1.2401, -1.5406, -0.3410],\n",
       "          [ 1.8970, -0.6617, -0.4737,  ...,  0.5683, -0.8601, -0.2478],\n",
       "          [ 1.2300, -1.3174,  0.6251,  ...,  0.4947, -1.0104, -0.3713],\n",
       "          ...,\n",
       "          [ 1.2679, -1.1703,  0.7929,  ...,  1.3577, -1.7963, -0.2717],\n",
       "          [ 1.0561, -1.1921, -0.1959,  ..., -0.0051, -1.7257,  0.2141],\n",
       "          [-0.2538, -1.0144,  0.0824,  ...,  0.3531, -1.0244, -1.6607]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Dataset_loader(annotations_file=\"test.csv\",img_dir=\"./new_images\")\n",
    "test.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Neural Network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(43200,2000)\n",
    "        self.fc2 = nn.Linear(2000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 500)\n",
    "        self.fc4 = nn.Linear(500, 100)\n",
    "        self.fc5 = nn.Linear(100, 5)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        x =F.log_softmax(self.fc5(x))\n",
    "        return x\n",
    "image_net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=43200, out_features=2000, bias=True)\n",
       "  (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "  (fc3): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (fc4): Linear(in_features=500, out_features=100, bias=True)\n",
       "  (fc5): Linear(in_features=100, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Text Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(24192,5000)\n",
    "        self.fc2 = nn.Linear(5000, 3000)\n",
    "        self.fc3 = nn.Linear(3000, 1500)\n",
    "        self.fc4 = nn.Linear(1500, 500)\n",
    "        self.fc5 = nn.Linear(500, 100)\n",
    "        self.fc6 = nn.Linear(100, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "\n",
    "        x =self.fc6(x)\n",
    "        return x\n",
    "nettext = TextNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joining_models(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(Joining_models, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.classifier = nn.Linear(10,12)\n",
    "        \n",
    "        self.output1 = nn.Linear(12, 2)\n",
    "        self.output2 = nn.Linear(12, 2)\n",
    "        self.output3 = nn.Linear(12, 2)\n",
    "        self.output4 = nn.Linear(12, 2)\n",
    "        self.output5 = nn.Linear(12, 3)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.model1(x1)\n",
    "        x2 = self.model2(x2)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        output1 = self.output1(x)\n",
    "        output2 = self.output2(x)\n",
    "        output3 = self.output3(x)\n",
    "        output4 = self.output4(x)\n",
    "        output5 = self.output5(x)\n",
    "        \n",
    "        return output1,output2,output3,output4,output5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmodel=Joining_models(image_net,nettext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(finalmodel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-69897222bdc2>:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x =F.log_softmax(self.fc5(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 2.352\n",
      "[1,     3] loss: 962.234\n",
      "[1,     5] loss: 328.815\n",
      "[1,     7] loss: 161.163\n",
      "[1,     9] loss: 50.567\n",
      "[1,    11] loss: 104.275\n",
      "[1,    13] loss: 47.805\n",
      "[1,    15] loss: 77.046\n",
      "[1,    17] loss: 46.591\n",
      "[1,    19] loss: 49.413\n",
      "[1,    21] loss: 19.407\n",
      "[1,    23] loss: 27.702\n",
      "[1,    25] loss: 22.350\n",
      "[1,    27] loss: 14.961\n",
      "[1,    29] loss: 10.526\n",
      "[1,    31] loss: 11.677\n",
      "[1,    33] loss: 10.104\n",
      "[1,    35] loss: 7.505\n",
      "[1,    37] loss: 6.296\n",
      "[1,    39] loss: 5.690\n",
      "[1,    41] loss: 4.494\n",
      "[1,    43] loss: 4.888\n",
      "[1,    45] loss: 4.104\n",
      "[1,    47] loss: 4.332\n",
      "[1,    49] loss: 3.936\n",
      "[1,    51] loss: 3.732\n",
      "[1,    53] loss: 3.872\n",
      "[1,    55] loss: 3.773\n",
      "[1,    57] loss: 3.690\n",
      "[1,    59] loss: 3.616\n",
      "[1,    61] loss: 3.629\n",
      "[1,    63] loss: 3.659\n",
      "[1,    65] loss: 3.582\n",
      "[2,     1] loss: 2.103\n",
      "[2,     3] loss: 3.595\n",
      "[2,     5] loss: 3.573\n",
      "[2,     7] loss: 3.603\n",
      "[2,     9] loss: 3.603\n",
      "[2,    11] loss: 3.772\n",
      "[2,    13] loss: 3.363\n",
      "[2,    15] loss: 3.597\n",
      "[2,    17] loss: 3.546\n",
      "[2,    19] loss: 3.543\n",
      "[2,    21] loss: 3.461\n",
      "[2,    23] loss: 3.540\n",
      "[2,    25] loss: 3.479\n",
      "[2,    27] loss: 3.641\n",
      "[2,    29] loss: 3.346\n",
      "[2,    31] loss: 3.439\n",
      "[2,    33] loss: 3.409\n",
      "[2,    35] loss: 3.440\n",
      "[2,    37] loss: 3.425\n",
      "[2,    39] loss: 3.416\n",
      "[2,    41] loss: 3.401\n",
      "[2,    43] loss: 3.466\n",
      "[2,    45] loss: 3.455\n",
      "[2,    47] loss: 3.553\n",
      "[2,    49] loss: 3.499\n",
      "[2,    51] loss: 3.551\n",
      "[2,    53] loss: 3.380\n",
      "[2,    55] loss: 3.509\n",
      "[2,    57] loss: 3.522\n",
      "[2,    59] loss: 3.343\n",
      "[2,    61] loss: 3.399\n",
      "[2,    63] loss: 3.539\n",
      "[2,    65] loss: 3.612\n",
      "[3,     1] loss: 1.893\n",
      "[3,     3] loss: 3.526\n",
      "[3,     5] loss: 3.663\n",
      "[3,     7] loss: 3.567\n",
      "[3,     9] loss: 3.423\n",
      "[3,    11] loss: 3.682\n",
      "[3,    13] loss: 3.704\n",
      "[3,    15] loss: 3.538\n",
      "[3,    17] loss: 3.524\n",
      "[3,    19] loss: 3.548\n",
      "[3,    21] loss: 3.465\n",
      "[3,    23] loss: 3.472\n",
      "[3,    25] loss: 3.470\n",
      "[3,    27] loss: 3.668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-5549c581f6a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtwo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput5\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfinalmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-9d610a7bdb48>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-69897222bdc2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten all dimensions except batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs,text, labels,two,three,four,five = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs,output2,output3,output4,output5= finalmodel(inputs,text)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss += criterion(output2, two)\n",
    "        loss += criterion(output3, three)\n",
    "        loss += criterion(output4, four)\n",
    "        loss += criterion(output5, five)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 0:   \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-69897222bdc2>:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x =F.log_softmax(self.fc5(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  = 77 percent\n",
      "Accuracy  = 73 percent\n",
      "Accuracy  = 47 percent\n",
      "Accuracy  = 65 percent\n",
      "Accuracy  = 53 percent\n"
     ]
    }
   ],
   "source": [
    "correct1 = 0\n",
    "correct2 = 0\n",
    "correct3 = 0\n",
    "correct4 = 0\n",
    "correct5 = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs,text, labels,two,three,four,five= data\n",
    "        outputs,output2,output3,output4,output5 = finalmodel(inputs,text)\n",
    "        _, predicted1 = torch.max(outputs.data, 1)\n",
    "        _, predicted2 = torch.max(output2.data, 1)\n",
    "        _, predicted3 = torch.max(output3.data, 1)\n",
    "        _, predicted4 = torch.max(output4.data, 1)\n",
    "        _, predicted5 = torch.max(output5.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct1 += (predicted1 == labels).sum().item()\n",
    "        correct2 += (predicted2 == two).sum().item()\n",
    "        correct3 += (predicted3 == three).sum().item()\n",
    "        correct4 += (predicted4 == four).sum().item()\n",
    "        correct5 += (predicted5 == five).sum().item()\n",
    "\n",
    "print(f'Accuracy  = {100 * correct1 // total } percent')\n",
    "print(f'Accuracy  = {100 * correct2 // total } percent')\n",
    "print(f'Accuracy  = {100 * correct3 // total } percent')\n",
    "print(f'Accuracy  = {100 * correct4 // total } percent')\n",
    "print(f'Accuracy  = {100 * correct5 // total } percent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './Ai_project.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "for X, y, z in validate_dataloader:\n",
    "        yhat = finalmodel(X,y)\n",
    "        break\n",
    "make_dot(yhat, params=dict(list(finalmodel.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
